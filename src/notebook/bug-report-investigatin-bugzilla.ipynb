{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Bug Reports Resolution on Bugzilla\n",
    "\n",
    "This Jupyter Notebook has the code for reproducing the experiment used in the master thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# evaluate random forest algorithm for classification\n",
    "from numpy import mean, std\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "import pickle\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# plot\n",
    "from matplotlib import pyplot as plt\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score, confusion_matrix, accuracy_score, recall_score, f1_score, plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# data balancing\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = None\n",
    "train = pd.read_csv('../data/train_balnced_no_test.csv')\n",
    "test = pd.read_csv('../data/test_umbalanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing some unwatend features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['status_RESOLVED', 'status_VERIFIED', 'changes_status', 'changes_resolution'], axis=1)\n",
    "test = test.drop(['status_RESOLVED', 'status_VERIFIED', 'changes_status', 'changes_resolution'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.copy(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filing the null values whit empty string\n",
    "\n",
    "data['description'].fillna('', inplace=True)\n",
    "data['total_words_desc'] = data.apply(lambda row: len(list(tokenize(row['description']))), axis=1)\n",
    "\n",
    "data['summary'].fillna('', inplace=True)\n",
    "data['total_words_summary'] = data.apply(lambda row: len(list(tokenize(row['summary']))), axis=1)\n",
    "\n",
    "# removing features that have mostly empty values\n",
    "data.drop(\"type\", inplace=True, axis=1)\n",
    "data.drop(\"flags\", inplace=True, axis=1)\n",
    "data.drop(\"assigned_to\", inplace=True, axis=1)\n",
    "data.drop(\"creator\", inplace=True, axis=1)\n",
    "data.drop(\"description\", inplace=True, axis=1)\n",
    "data.drop(\"summary\", inplace=True, axis=1)\n",
    "data.drop(\"id\", inplace=True, axis=1)\n",
    "data.drop(\"creation_time\", inplace=True, axis=1)\n",
    "data.drop(\"last_change_time\", inplace=True, axis=1)\n",
    "\n",
    "# Target feature\n",
    "target_feature = data[[\"resolution\"]]\n",
    "data.drop(\"resolution\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming non-numeric features in dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Balancing\n",
    "\n",
    "For balancind the data the follwoing chunks must be executed.\n",
    "X_ros - data balanced\n",
    "y_ros - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "X = data.drop('label', axis=1)\n",
    "X_ros, y_ros = ros.fit_resample(X, data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util\n",
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    return [pd.read_csv(path + \"train.csv\"), pd.read_csv(path + \"test.csv\")]\n",
    "\n",
    "\n",
    "def show_distribution_graph(df):\n",
    "    df = df['label'].value_counts().to_frame()\n",
    "    df = df.reset_index().rename(columns={\"index\": \"Resolution\", \"label\": \"Total\"})\n",
    "\n",
    "    return alt.Chart(df).mark_bar().encode(\n",
    "        x=alt.X('Resolution', sort='-y'),\n",
    "        y='Total',\n",
    "        color = alt.value(\"#ac97b4\")\n",
    "    )\n",
    "\n",
    "\n",
    "def model_confusion_matrix(model, x_test, y_test, classes):\n",
    "    return plot_confusion_matrix(model, x_test, y_test, labels=classes, cmap=plt.cm.Blues, xticks_rotation = \"vertical\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred, y_test):\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average='weighted')\n",
    "    recall = recall_score(y_test, pred, average='weighted')\n",
    "    f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "    return {\"Metrics\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"], \"Scores\": [accuracy, precision, recall, f1]}\n",
    "\n",
    "\n",
    "def print_metrics(model_name, metrics):\n",
    "    print(f\"{model_name} Metrics:\\n\")\n",
    "    for i in range(4):\n",
    "        print(f\"{metrics['Metrics'][i]} score is:\\t{round(metrics['Scores'][i] * 100,2)}%\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def compute_metrics_per_class(pred, y_test):\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    # Print the precision and recall, among other metrics\n",
    "    print(metrics.classification_report(y_test, pred, digits=2))\n",
    "\n",
    "\n",
    "def get_metrics(model, model_name, x_train, y_train, x_test, y_test, classes):\n",
    "    model.fit(x_train, y_train)\n",
    "    pred = model.predict(x_test)\n",
    "\n",
    "    metrics = compute_metrics(pred,  y_test)\n",
    "    print_metrics(model, model_name, metrics, x_test, y_test, classes)\n",
    "    compute_metrics_per_class(pred, y_test)\n",
    "\n",
    "    return metrics['Scores']\n",
    "\n",
    "\n",
    "def get_metric_data(models, models_names):\n",
    "    # models: [NB_metrics,LG_metrics,DT_metrics,RF_metrics, GB_metrics]\n",
    "    # TODO: fix this method\n",
    "    metrics = np.array(models).flatten()\n",
    "    metrics = list(map(lambda x: x*100, metrics))\n",
    "\n",
    "    data = {\"Metric\":\n",
    "            ['Accuracy', 'Precision', 'Recall', 'F1 Score'] * 5,\n",
    "            \"Metric Score\": metrics,\n",
    "            \"Model\":\n",
    "            ['Naive Bayes'] * 4 +\n",
    "            ['Logistic Regression'] * 4 +\n",
    "            ['Decision Tree'] * 4 +\n",
    "            ['Random Forest'] * 4 +\n",
    "            ['Gradient Boosting'] * 4\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def plot_metric_graph(data):\n",
    "    g = sns.catplot(\n",
    "        data=data,\n",
    "        kind=\"bar\", x=\"Metric\", y=\"Metric Score\", hue=\"Model\",\n",
    "        ci=\"sd\", alpha=.6, height=6\n",
    "    )\n",
    "    g.set(ylim=(0, 100))\n",
    "    g.despine(left=True)\n",
    "\n",
    "\n",
    "def get_params(model_name):\n",
    "    if (model_name == \"Logistic Regression\"):\n",
    "        return { 'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0, 100000.0] }\n",
    "    elif (model_name == \"Gradient Boosting\"):\n",
    "        return {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [3, 8],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 5],\n",
    "            'max_features': [None, 5],\n",
    "            'subsample': [0.5, 1]\n",
    "            }\n",
    "    else:\n",
    "        return { 'max_depth': [None, 15, 35, 50], 'max_leaf_nodes': [None, 250, 500, 750, 1000, 5000]}\n",
    "\n",
    "def print_best_params(model_name, grid, folds):\n",
    "    print(f'Hyperparams of {model_name}:\\n')\n",
    "    print(f'Got accuracy score of {grid.best_score_} in {folds}-fold')\n",
    "    if (model_name == \"Logistic Regression\"):\n",
    "        print(f'Best C: {grid.best_params_[\"C\"]}')\n",
    "    elif (model_name == \"Gradient Boosting\"):\n",
    "        print(f'Best max depth: {grid.best_params_[\"max_depth\"]}. Best number of estimators: {grid.best_params_[\"n_estimators\"]}')\n",
    "        print(f'Best min sample split: {grid.best_params_[\"min_samples_split\"]}. Best min sample leaf: {grid.best_params_[\"min_samples_leaf\"]}')\n",
    "        print(f'Best max features: {grid.best_params_[\"max_features\"]}. Best subsample: {grid.best_params_[\"subsample\"]}')\n",
    "    else:\n",
    "        print(f'Best depth: {grid.best_params_[\"max_depth\"]}. Best number of leafs: {grid.best_params_[\"max_leaf_nodes\"]}')\n",
    "\n",
    "def get_tuned_metrics(model, model_name, folds, x_train, y_train, x_test, y_test, classes):\n",
    "    params = get_params(model_name)\n",
    "\n",
    "    grid = GridSearchCV(model, params, cv = folds)\n",
    "    grid.fit(x_train, y_train)\n",
    "    pred = grid.predict(x_test)\n",
    "\n",
    "    print_best_params(model_name, grid, folds)\n",
    "    metrics = compute_metrics(pred, y_test)\n",
    "    print_metrics(grid, model_name, metrics, x_test, y_test, classes)\n",
    "    return metrics['Scores'], pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced train distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_distribution_graph(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_distribution_graph(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = GaussianNB()\n",
    "LG = LogisticRegression(max_iter=5000)\n",
    "DT = DecisionTreeClassifier()\n",
    "RF = RandomForestClassifier(random_state=42)\n",
    "GB = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop('label', axis=1)\n",
    "x_test = test.drop('label', axis=1)\n",
    "y_train = train['label']\n",
    "y_test = test['label']\n",
    "classes = train['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_metrics = get_metrics(NB, \"Naive Bayes\", x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG_metrics = get_metrics(LG, \"Logistic Regression\", x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_metrics = get_metrics(DT, \"Decision Tree\", x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_metrics = get_metrics(RF, \"Random Forest\", x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_metrics = get_metrics(GB, \"Gradient Boosting\", x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scores_NB = cross_val_score(NB, x_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores_NB), std(n_scores_NB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scores_LG = cross_val_score(LG, x_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores_LG), std(n_scores_LG)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest\n",
    "TODO: instance the model again and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scores_RF = cross_val_score(RF, x_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores_RF), std(n_scores_RF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#took too much time\n",
    "#n_scores_GB = cross_val_score(GB, x_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "#print('Accuracy: %.3f (%.3f)' % (mean(n_scores_GB), std(n_scores_GB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Finetunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tuned_metrics, nb_pred = get_tuned_metrics(NB, \"Naive Bayes\", 10, x_train, y_train, x_test, y_test, classes)\n",
    "pickle.dump(NB_tuned_metrics, open('./data/models/NB_tuned_metrics.pkl', 'wb'))\n",
    "pickle.dump(nb_pred, open('./data/models/nb_pred.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG_tuned_metrics, lg_pred = get_tuned_metrics(LG, \"Logistic Regression\", 10, x_train, y_train, x_test, y_test, classes)\n",
    "pickle.dump(LG_tuned_metrics, open('./data/models/LG_tuned_metrics.pkl', 'wb'))\n",
    "pickle.dump(lg_pred, open('./data/models/lg_pred.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_tuned_metrics, dt_pred = get_tuned_metrics(DT, \"Decision Tree\", 10, x_train, y_train, x_test, y_test, classes)\n",
    "pickle.dump(DT_tuned_metrics, open('./data/models/DT_tuned_metrics.pkl', 'wb'))\n",
    "pickle.dump(dt_pred, open('./data/models/dt_pred.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_tuned_metrics, rf_pred = get_tuned_metrics(RF, 'RandomForest', 10, x_train, y_train, x_test, y_test, classes)\n",
    "pickle.dump(RF_tuned_metrics, open('./data/models/RF_tuned_metrics.pkl', 'wb'))\n",
    "pickle.dump(rf_pred, open('./data/models/rf_pred.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_tuned_metrics, gb_pred = get_tuned_metrics(GB, 'Gradient Boosting', 10, x_train, y_train, x_test, y_test, classes)\n",
    "pickle.dump(GB_tuned_metrics, open('./data/models/GB_tuned_metrics.pkl', 'wb'))\n",
    "pickle.dump(gb_pred, open('./data/models/gb_pred.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring 4 differents groupings\n",
    "\n",
    "**4 Classes**\n",
    "- FIXED: FIXED\n",
    "- INCOMPLETE: INCOMPLETE, WORKSFORME, and INVALID\n",
    "- IGNORED: WONTFIX and DUPLICATED\n",
    "- INACTIVE: INACTIVE and MOVED\n",
    "\n",
    "**3 Classes**\n",
    "- FIXED: FIXED\n",
    "- INVALID: INVALID, WORKSFORME, DUPLICATED, and INCOMPLETE\n",
    "- IGNORED: MOVED, WONTFIX, and INACTIVE\n",
    "\n",
    "**2 Classes**\n",
    "- FIXED: FIXED\n",
    "- INCOMPLETE: INCOMPLETE, INACTIVE, WORKSFORME, INVALID, MOVED, DUPLICATE, WONTFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GROUPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_classes = train.copy()\n",
    "test_new_classes = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "train_new_classes['label'] = train_new_classes['label'].replace(['MOVED', 'INACTIVE'], 'INACTIVE')\n",
    "train_new_classes['label'] = train_new_classes['label'].replace(['WONTFIX', 'DUPLICATE'], 'IGNORED')\n",
    "train_new_classes['label'] = train_new_classes['label'].replace(['INCOMPLETE', 'WORKSFORME', 'INVALID'], 'INCOMPLETE')\n",
    "\n",
    "#Test\n",
    "test_new_classes['label'] = test_new_classes['label'].replace(['MOVED', 'INACTIVE'], 'INACTIVE')\n",
    "test_new_classes['label'] = test_new_classes['label'].replace(['WONTFIX', 'DUPLICATE'], 'IGNORED')\n",
    "test_new_classes['label'] = test_new_classes['label'].replace(['INCOMPLETE', 'WORKSFORME', 'INVALID'], 'INCOMPLETE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "train_new_classes['label'] = train_new_classes['label'].replace(['INVALID', 'WORKSFORME', 'DUPLICATED', 'INCOMPLETE'], 'INVALID')\n",
    "train_new_classes['label'] = train_new_classes['label'].replace(['MOVED', 'WONTFIX', 'INACTIVE'], 'IGNORED')\n",
    "\n",
    "#Test\n",
    "test_new_classes['label'] = test_new_classes['label'].replace(['INVALID', 'WORKSFORME', 'DUPLICATED', 'INCOMPLETE'], 'INVALID')\n",
    "test_new_classes['label'] = test_new_classes['label'].replace(['MOVED', 'WONTFIX', 'INACTIVE'], 'IGNORED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "train_new_classes['label'] = train_new_classes['label'].replace(['INVALID', 'WORKSFORME', 'DUPLICATED', 'INCOMPLETE', 'MOVED', 'WONTFIX', 'INACTIVE'], 'INCOMPLETE')\n",
    "\n",
    "#Test\n",
    "test_new_classes['label'] = test_new_classes['label'].replace(['INVALID', 'WORKSFORME', 'DUPLICATED', 'INCOMPLETE', 'MOVED', 'WONTFIX', 'INACTIVE'], 'INCOMPLETE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing Random Forest with the new classes\n",
    "get_metrics(model, model_name, x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new_classes = train_new_classes.drop('label', axis=1)\n",
    "x_test_new_classes = test_new_classes.drop('label', axis=1)\n",
    "y_train_new_classes = train_new_classes['label']\n",
    "y_test_new_classes = test_new_classes['label']\n",
    "classes_new_classes = train_new_classes['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_new_classes = RandomForestClassifier(random_state=42)\n",
    "RF_metrics_new_classes = get_metrics(RF_new_classes, \"Random Forest\", x_train_new_classes, y_train_new_classes, x_test_new_classes, y_test_new_classes, classes_new_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = RF.fit(x_train, y_train)\n",
    "pred = RF.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda = test.copy()\n",
    "data_eda['predicted'] = pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check whether the FIXED classifications was right we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classification(row):\n",
    "    if ((row['label'] == row['predicted']) & (row['label'] == 'FIXED')):\n",
    "        return 'right'\n",
    "    elif ((row['label'] != row['predicted']) & (row['label'] == 'FIXED')):\n",
    "        return 'wrong'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda['classification'] = data_eda.apply(lambda row: eval_classification(row), axis=1)\n",
    "data_eda.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda.to_csv('../data/data_eda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda = pd.read_csv('../data/data_eda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(20.7, 12.27)})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "ax = sns.boxplot(x='comment_count',y='classification',data=data_eda).set(xlim=(0, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_eda[data_eda['classification'] == 'wrong'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INCOMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tamanho dos comentários\n",
    "- Descrição dos comentários\n",
    "- Quantidade de palavras impactam?\n",
    "- Qual a relação dos comentários com outras variáveis?\n",
    "- Os classificados de forma correta têm mais mudanças(total_changes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda[['label', 'total_users_commenting']].groupby('label').mean().plot.bar(y='total_users_commenting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda_plot = data_eda[['label', 'total_users_commenting']].groupby('label').median()\n",
    "data_eda_plot.plot.bar(y='total_users_commenting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df['median'].plot(ax=ax, marker='o', ls='-', color='#4C9A2A', alpha = 0.9)\n",
    "df['total_users_commenting'].plot(kind='bar', ax=ax, alpha=0.7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE: \n",
    "\n",
    "Label  | Mean | Median | R-Median | R-Mean | W-Median | W-Mean | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_analysis_feature(df, feature):\n",
    "    analysis = pd.DataFrame()\n",
    "    median = data_eda[['label', feature]].groupby('label').median()[feature]\n",
    "    right_median = data_eda[data_eda['label'] == data_eda['predicted']][['label', feature]].groupby('label').median().rename(columns = {feature:'right_median'})\n",
    "    analysis = data_eda[data_eda['label'] != data_eda['predicted']][['label', feature]].groupby('label').median().rename(columns = {feature:'wrong_median'})\n",
    "\n",
    "    analysis['median'] = median.values\n",
    "    analysis['right_median'] = right_median['right_median']\n",
    "\n",
    "\n",
    "    mean = data_eda[['label', feature]].groupby('label').mean()[feature]\n",
    "    right_mean = data_eda[data_eda['label'] == data_eda['predicted']][['label', feature]].groupby('label').mean().rename(columns = {feature:'right_mean'})\n",
    "    analysis['wrong_mean'] = data_eda[data_eda['label'] != data_eda['predicted']][['label', feature]].groupby('label').mean().rename(columns = {feature:'wrong_mean'})\n",
    "\n",
    "    analysis['mean'] = mean.values\n",
    "    analysis['right_mean'] = right_mean['right_mean']\n",
    "\n",
    "    return analysis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_analysis_feature_mean(df, feature):\n",
    "    analysis = pd.DataFrame()\n",
    "    mean = data_eda[['label', feature]].groupby('label').mean()[feature]\n",
    "    right_mean = data_eda[data_eda['label'] == data_eda['predicted']][['label', feature]].groupby('label').mean().rename(columns = {feature:'right_mean'})\n",
    "    analysis = data_eda[data_eda['label'] != data_eda['predicted']][['label', feature]].groupby('label').mean().rename(columns = {feature:'wrong_mean'})\n",
    "\n",
    "    analysis['median'] = mean.values\n",
    "    analysis['right_mean'] = right_mean['right_mean']\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda[(data_eda['label'] == 'INCOMPLETE')  &  (data_eda['predicted'] == 'WORKSFORME')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking models classification by features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_total_words_desc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'total_words_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_total_words_summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'total_words_summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_total_changes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'total_changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_comment_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'comment_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_total_users_commenting_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'total_users_commenting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_total_users_changes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'total_users_changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_total_comments_by_author_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'total_comments_by_author')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_total_attachment_comments_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'total_attachment_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_severity_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_analysis_feature(data_eda, 'severity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Fine tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=10)\n",
    "}\n",
    "\n",
    "param_grid_nb\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)\n",
    "nbModel_grid.fit(x_train, y_train)\n",
    "print(nbModel_grid.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
