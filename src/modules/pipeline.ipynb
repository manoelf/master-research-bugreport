{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from src.modules.pipeline.balancing import Balancing\n",
    "from src.modules.pipeline.cross_validation import CrossValidation\n",
    "from src.modules.pipeline.finetunning import Finetunning\n",
    "from src.modules.preprocess.preprocess import Preprocess\n",
    "from src.modules.util.constant import Features, Model, ModelName as mn\n",
    "from src.modules.util.helper_metrics import MetricsHelper as mh\n",
    "from src.modules.util.util import Util as util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lorenna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = Model.NB\n",
    "LG = Model.LG\n",
    "DT = Model.DT\n",
    "RF = Model.RF\n",
    "GB = Model.GB\n",
    "CV = Model.CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BALANCE_PATH = '../data/balanced/balanced_data.csv'\n",
    "\n",
    "data = pd.read_csv('../data/bg_results.csv', engine='python', quoting=3, header=0, sep='ยง')\n",
    "data.drop(columns=Features.train_test_features, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.copy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filing the null values whit empty string\n",
    "\n",
    "data['summary'].fillna('', inplace=True)\n",
    "data['total_words_summary'] = data.apply(lambda row: len(list(nltk.word_tokenize(row['summary']))), axis=1)\n",
    "\n",
    "data['description'].fillna('', inplace=True)\n",
    "data['total_words_description'] = data.apply(lambda row: len(list(nltk.word_tokenize(row['description']))), axis=1)\n",
    "\n",
    "data.drop(columns=Features.features, inplace=True, axis=1)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For balancind the data the follwoing chunks must be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = Balancing.oversample(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving balanced data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(BALANCE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN, TEST = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TRAIN.drop('resolution', axis = 1)\n",
    "y_train = TRAIN['resolution']\n",
    "\n",
    "test = TEST.drop('resolution', axis=1)\n",
    "y_test = TEST['resolution']\n",
    "\n",
    "classes = TRAIN['resolution'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "text_columns = train.select_dtypes(include=['object']).columns\n",
    "x_train_vectorized = vectorizer.fit_transform(train[text_columns])\n",
    "\n",
    "text_columns = test.select_dtypes(include=['object']).columns\n",
    "x_test_vectorized =  vectorizer.transform(test[text_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vectorized = pd.DataFrame(x_test_vectorized.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "x_train_vectorized = pd.DataFrame(x_train_vectorized.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric = train.drop(text_columns, axis=1)\n",
    "test_numeric = test.drop(text_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_numeric.join(x_train_vectorized, lsuffix='_numeric', rsuffix='_vectorize')\n",
    "x_test = test_numeric.join(x_test_vectorized, lsuffix='_numeric', rsuffix='_vectorize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nomalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# x_train_normalized = scaler.fit_transform(x_train)\n",
    "# x_test_normalized = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x_train[c].apply(lambda x: print(x, c) if (str(x).isalpha() and str(x) not in ['nan', 'True', 'False']) else None) for c in x_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m NB_metrics \u001b[39m=\u001b[39m util()\u001b[39m.\u001b[39;49mget_metrics(NB, mn\u001b[39m.\u001b[39;49mNB, x_train, y_train, x_test, y_test, classes)\n",
      "File \u001b[0;32m~/manoel-master/master-research-bugreport/src/modules/../../src/modules/util/util.py:60\u001b[0m, in \u001b[0;36mUtil.get_metrics\u001b[0;34m(self, model, model_name, x_train, y_train, x_test, y_test, classes)\u001b[0m\n\u001b[1;32m     57\u001b[0m pred \u001b[39m=\u001b[39m tm()\u001b[39m.\u001b[39mpredict_model(model, x_test)\n\u001b[1;32m     59\u001b[0m predicted_probabilities \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(x_test)[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m metrics \u001b[39m=\u001b[39m mh()\u001b[39m.\u001b[39;49mcompute_metrics(pred, y_test, predicted_probabilities)\n\u001b[1;32m     62\u001b[0m final_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m initial_time\n\u001b[1;32m     64\u001b[0m ShowMetrics()\u001b[39m.\u001b[39mshow_metrics(model_name, metrics)\n",
      "File \u001b[0;32m~/manoel-master/master-research-bugreport/src/modules/../../src/modules/util/helper_metrics.py:26\u001b[0m, in \u001b[0;36mMetricsHelper.compute_metrics\u001b[0;34m(self, pred, y_test, predict_prob, average)\u001b[0m\n\u001b[1;32m     23\u001b[0m f1 \u001b[39m=\u001b[39m f1_score(y_test, pred, average\u001b[39m=\u001b[39maverage)\n\u001b[1;32m     25\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(y_test)\n\u001b[0;32m---> 26\u001b[0m y_true \u001b[39m=\u001b[39m label_binarize(y_test, classes\u001b[39m=\u001b[39;49mclasses)\n\u001b[1;32m     28\u001b[0m y_scores \u001b[39m=\u001b[39m predict_prob\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(classes))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:556\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[0;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[1;32m    553\u001b[0m y \u001b[39m=\u001b[39m column_or_1d(y)\n\u001b[1;32m    555\u001b[0m \u001b[39m# pick out the known labels from y\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m y_in_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49min1d(y, classes)\n\u001b[1;32m    557\u001b[0m y_seen \u001b[39m=\u001b[39m y[y_in_classes]\n\u001b[1;32m    558\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msearchsorted(sorted_class, y_seen)\n",
      "File \u001b[0;32m~/anaconda3/envs/manoel-master/lib/python3.11/site-packages/numpy/lib/arraysetops.py:733\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    731\u001b[0m         mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(ar1), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    732\u001b[0m         \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m ar2:\n\u001b[0;32m--> 733\u001b[0m             mask \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m (ar1 \u001b[39m==\u001b[39m a)\n\u001b[1;32m    734\u001b[0m     \u001b[39mreturn\u001b[39;00m mask\n\u001b[1;32m    736\u001b[0m \u001b[39m# Otherwise use sorting\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NB_metrics = util().get_metrics(NB, mn.NB, x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG_metrics = util().get_metrics(LG, mn.LG, x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_metrics = util().get_metrics(DT, mn.DT, x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_metrics = util().get_metrics(RF, mn.RF, x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_metrics = util().get_metrics(GB, mn.GB, x_train, y_train, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_cv = CrossValidation().get_cross_validation_result(NB, x_train, y_train)\n",
    "NB_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG_cv = CrossValidation().get_cross_validation_result(LG, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_cv = CrossValidation().get_cross_validation_result(DT, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_cv = CrossValidation().get_cross_validation_result(RF, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_cv = CrossValidation().get_cross_validation_result(GB, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Util.get_params() got an unexpected keyword argument 'deep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Finetunning()\u001b[39m.\u001b[39;49mmodel_finetuning(NB, mn\u001b[39m.\u001b[39;49mNB, x_train, y_train, x_test, y_test, classes, \u001b[39m'\u001b[39;49m\u001b[39m./data/models/naive_bayers/NB_tuned_metrics.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m./data/models/naive_bayers/NB_pred.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/manoel-master/master-research-bugreport/src/modules/../../src/modules/pipeline/finetunning.py:12\u001b[0m, in \u001b[0;36mFinetunning.model_finetuning\u001b[0;34m(self, model, model_name, x_train, y_train, x_test, y_test, classes, model_turned_metrics_path, model_pred_path, folds)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel_finetuning\u001b[39m(\u001b[39mself\u001b[39m, model: Model, model_name: ModelName, x_train, y_train, x_test, y_test, classes, model_turned_metrics_path, model_pred_path, folds\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     model_tuned_metrics, model_pred \u001b[39m=\u001b[39m util()\u001b[39m.\u001b[39;49mget_tuned_metrics(model, model_name, folds, x_train, y_train, x_test, y_test, classes)\n\u001b[1;32m     14\u001b[0m     pickle\u001b[39m.\u001b[39mdump(model_tuned_metrics, \u001b[39mopen\u001b[39m(model_turned_metrics_path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m     pickle\u001b[39m.\u001b[39mdump(model_pred, \u001b[39mopen\u001b[39m(model_pred_path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/manoel-master/master-research-bugreport/src/modules/../../src/modules/util/util.py:92\u001b[0m, in \u001b[0;36mUtil.get_tuned_metrics\u001b[0;34m(self, model, model_name, folds, x_train, y_train, x_test, y_test, classes)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tuned_metrics\u001b[39m(\u001b[39mself\u001b[39m, model, model_name, folds, x_train, y_train, x_test, y_test, classes):\n\u001b[0;32m---> 92\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(model_name, deep\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     94\u001b[0m     grid \u001b[39m=\u001b[39m GridSearchCV(model, params, cv \u001b[39m=\u001b[39m folds)\n\u001b[1;32m     95\u001b[0m     grid\u001b[39m.\u001b[39mfit(x_train, y_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: Util.get_params() got an unexpected keyword argument 'deep'"
     ]
    }
   ],
   "source": [
    "Finetunning().model_finetuning(NB, mn.NB, x_train, y_train, x_test, y_test, classes, './data/models/naive_bayers/NB_tuned_metrics.pkl', './data/models/naive_bayers/NB_pred.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finetunning().model_finetuning(LG, mn.LG, x_train, y_train, x_test, y_test, classes, './data/models/logistic_regression/LG_tuned_metrics.pkl', './data/models/logistic_regression/LG_pred.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finetunning().model_finetuning(DT, mn.DT, x_train, y_train, x_test, y_test, classes, './data/models/decision_tree/DT_tuned_metrics.pkl', './data/models/decision_tree/DT_pred.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finetunning().model_finetuning(RF, mn.RF, x_train, y_train, x_test, y_test, classes, './data/models/random_forest/RF_tuned_metrics.pkl', './data/models/random_forest/RF_pred.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finetunning().model_finetuning(GB, mn.GB, x_train, y_train, x_test, y_test, classes, './data/models/gradient_boosting/GB_tuned_metrics.pkl', './data/models/gradient_boosting/GB_pred.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util().save_result(NB_metrics, NB_time, mn.NB)\n",
    "util().save_result(LG_metrics, LG_time, mn.LG)\n",
    "util().save_result(DT_metrics, DT_time, mn.DT)\n",
    "util().save_result(RF_metrics, RF_time, mn.RF)\n",
    "util().save_result(GB_metrics, GB_time, mn.GB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
